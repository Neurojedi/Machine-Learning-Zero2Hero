{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d0e011",
   "metadata": {
    "id": "65d0e011"
   },
   "source": [
    "# Linear Models in Machine Learning on Python - Support Vector Machines 4 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f129cfe",
   "metadata": {
    "id": "1f129cfe"
   },
   "source": [
    "Welcome to the last notebook on SVM. In this notebook, we will learn four regression algorithms. I will only talk about one of them in depth because the other three are not that commonly used I think. Nevertheless, as always I will share extra documents in which you can find more about the mathematics behind them. Let's get started with the most important one, Support Vector Regression.\n",
    "\n",
    "Previously we used support vectors to separate two classes (or more classes by using some strategies). We can use the same support vectors to tune a line to fit a regression line. In this case, the algorithm is called Support Vector Regression. Let's find out how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f0ad94",
   "metadata": {
    "id": "17f0ad94"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c331b81",
   "metadata": {
    "id": "9c331b81"
   },
   "source": [
    "## Math behind SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a550c5",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/svm7.PNG\" width=\"350\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56101de",
   "metadata": {
    "id": "c56101de"
   },
   "source": [
    "Let's start with the picture above and introduce the most important difference compared to SVC. The algorithm has a region called $\\epsilon$-tube or $\\epsilon$-insensitive tube. The cost function we have is insensitive to the points inside that tube, this is the reason it is called an epsilon insensitive tube and insensitive cost. This concept of the tube is actually like the soft margin classifier because by using this tube we, to some extent, allow margin errors. Let's go through the algorithm step-by-step and see how it uses this cost function.\n",
    "\n",
    "\n",
    "1. The main objective of the algorithm is to find the narrowest tube while minimizing the loss function which can be defined as the distance between the prediction and the target.\n",
    "\n",
    "$$min_w=\\frac{1}{2}w^Tw$$\n",
    "\n",
    "2. We need to define constraints and add a loss term to the objective function above, just as we added hinge loss in Support Vector Classifier. This loss term is epsilon insensitive loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60551f1",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/svm8.PNG\" width=\"750\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a7c57",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905642e",
   "metadata": {
    "id": "0905642e"
   },
   "source": [
    "The function in the first plot is called epsilon insensitive loss and is defined as\n",
    "\n",
    "$$L_{\\epsilon }\\:\\left(y,f\\left(x,w\\right)\\right)=\\begin{cases}0&,\\:for\\:\\left|y-f\\left(x\\right)\\right|\\le \\:\\epsilon\\\\ \\:\\left(\\left|y-f\\left(x,w\\right)\\right|-\\epsilon\\right)&,\\:otherwise\\end{cases}$$\n",
    "\n",
    "and the second one is square insensitive loss and defined as\n",
    "\n",
    "$$L_{\\epsilon }\\:\\left(y,f\\left(x,w\\right)\\right)=\\begin{cases}0&,\\:for\\:\\left|y-f\\left(x\\right)\\right|\\le \\:\\epsilon\\\\ \\:\\left(\\left|y-f\\left(x,w\\right)\\right|-\\epsilon\\right)^2&,\\:otherwise\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8cae1",
   "metadata": {
    "id": "76e8cae1"
   },
   "source": [
    "3. The cost function penalizes points that are farther than $\\epsilon$ from the target.\n",
    "\n",
    "4. We also know that epsilon determines the width of the tube. In that case, as we talked about above in the beginning, we do not penalize points inside the tube and penalize points outside to tune our regression line. In fact, we are using the points outside the tube as our support vectors and optimizing the model. If you look at the first picture when we lower the width of the tube ( this means decreasing epsilon) we have a narrower tube which corresponds to a lower tolerance for errors and we get more support vectors. If we increase the width of the tube (this means increasing epsilon) we tolerate more errors and have fewer support vectors. \n",
    "\n",
    "### Primal Form of SVR\n",
    "\n",
    "By using the loss function above we can derive the primal form as follows.\n",
    "\n",
    "$$\\frac{1}{2}w^Tw+C\\cdot \\sum_{i=1}^m\\:\\zeta \\:_i+\\zeta _i^{\\ast}$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$y_i-w^Tx_i\\le \\:\\epsilon \\:+\\zeta ^{\\ast }_i\\:\\: for\\:\\: i=1,2,3,..,m$$\n",
    "\n",
    "$$w^Tx_i-y_i\\le \\epsilon +\\zeta _i\\:\\:\\: for\\:\\: i=1,2,3,..,m$$\n",
    "\n",
    "\n",
    "$$\\zeta _i,\\zeta _i^{\\ast }\\ge \\:0\\:\\: for\\:\\: i=1,2,3,..,m$$\n",
    "\n",
    "By using the Primal Problem we can derive the Lagrange function as follows\n",
    "\n",
    "$$L_p\\left(w,\\:\\zeta ^{\\ast },\\:\\zeta ,\\:\\:\\lambda ,\\:\\:\\lambda ^{\\ast },\\:\\alpha ,\\:\\alpha ^{\\ast }\\right)=$$\n",
    "\n",
    "\n",
    "$$\\frac{1}{2}w^Tw+C\\cdot \\:\\sum _{i=1}^m\\:\\zeta _i+\\zeta \\:_i^{\\ast }+\\sum_{i=1}^m\\alpha _i^{\\ast \\:}\\left(y_i-w^Tx_i-\\epsilon -\\zeta _i\\:^{\\ast \\:}\\right)+\\sum_{i=1}^m\\alpha _i\\left(-y_i+w^Tx_i-\\epsilon \\:-\\zeta _i\\:\\:\\right)-\\sum_{i=1}^m\\lambda \\:_i\\cdot \\zeta _i\\:+\\lambda _i^{\\ast \\:}\\cdot \\zeta _i\\:^{\\ast \\:}$$\n",
    "\n",
    "If we derivate the lagrangian with respect to each term in the function, we can get the conditions for dual form. I will not show these conditions here, however, you can find them in the paper that I shared at the end.\n",
    "\n",
    "### Dual Form of SVR\n",
    "\n",
    "Let's now get our dual form.\n",
    "\n",
    "$$L_D\\left(\\alpha ,\\:\\alpha ^{\\ast }\\right)=-\\epsilon \\cdot \\sum _{i=1}^{N_{sv}}\\:\\left(\\alpha _i\\:+\\alpha _i^{\\ast \\:}\\right)+\\sum _{i=1}^{N_{sv}}\\:\\left(\\alpha _i^{\\ast }-\\alpha \\:\\:_i\\right)y_i+\\frac{1}{2}\\sum _{j=1}^{N_{sv}}\\sum _{i=1}^{N_{sv}}\\left(\\alpha \\:_i^{\\ast \\:}-\\alpha _i\\right)\\left(\\alpha _j^{\\ast }-\\alpha _j\\right)x_i^Tx_j$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\\sum _{i=1}^{N_{sv}}\\:\\left(\\alpha _i^{\\ast \\:\\:}-\\alpha \\:_i\\right)=0\\:\\: and \\:\\:a_i,a_j\\:\\in \\left[0,C\\right]$$\n",
    "\n",
    "and finally, we can approximate the regression estimate by using the following equation.\n",
    "\n",
    "$$f\\left(x\\right)=\\sum _{i=1}^{N_{sv}}\\:\\left(\\alpha _i^{\\ast \\:\\:}-\\alpha \\:_i\\right)x_i^Tx\\:+b\\:\\:for\\:\\alpha \\:_i^{\\ast },\\alpha _i\\:\\in \\left[0,C\\right]$$\n",
    "\n",
    "\n",
    "Note: $N_{sv}$ = number of support vectors and don't forget that the dual form is a maximization problem, whereas, the primal form is a minimization problem.\n",
    "\n",
    "\n",
    "### Kernels\n",
    "\n",
    "We can use Kernels in SVR as we used them in SVC. The main idea is almost the same. We map input space into a higher dimensional space but this time we don't do it for getting a better classification score but we are doing it for getting a better regression line. Let's define the dual form of SVR with kernel function included:\n",
    "\n",
    "$$L_D\\left(\\alpha,\\: \\alpha^{\\ast \\:}\\right)=-\\epsilon \\cdot \\:\\sum _{i=1}^{N_{sv}}\\left(\\alpha \\:_i\\:+\\alpha _i^{\\ast }\\right)+\\sum_{i=1}^{N_{sv}}\\left(\\alpha _i^{\\ast }-\\alpha_i\\right)y_i+\\frac{1}{2}\\sum _{j=1}^{N_{sv}}\\sum _{i=1}^{N_{sv}}\\left(\\alpha_i^{\\ast}-\\alpha \\:_i\\right)\\left(\\alpha_j^{\\ast }-\\alpha _j\\right)K\\left(x_i,x_j\\right)$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\\sum_{i=1}^{N_{sv}}\\:\\left(\\alpha_i^{\\ast }-\\alpha_i\\right)=0,\\: and \\: a_i,a_j\\in \\:\\left[0,C\\right]$$\n",
    "\n",
    "Let's also define the Kernel function.\n",
    "\n",
    "$$K\\left(x_i,x\\right)=\\phi \\left(x_i\\right)\\phi \\left(x\\right)$$\n",
    "\n",
    "\n",
    "and finally we can estimate the regression line by using the following equation.\n",
    "\n",
    "$$f\\left(x\\right)=\\sum_{i=1}^{N_{sv}}\\left(\\alpha_i^{\\ast }-\\alpha_i\\right)K\\left(x_i,x\\right)+b$$\n",
    "\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "1. Firstly I will share two medium articles about SVR: [An Introduction to Support Vector Regression (SVR)](https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2) and [Support Vector Regression and it’s Mathematical Implementation](https://medium.com/swlh/support-vector-regression-and-its-mathematical-implementation-4800456e4878)\n",
    "2. I took almost all the equations from the following paper. The paper introduces the concepts and mathematics under the hood very well. I highly recommend reading it [Support Vector Regression](https://www.researchgate.net/publication/300719035_Support_Vector_Regression)\n",
    "3. Paul Scharater from the University of Minnesota has awesome lecture notes for Support Vector Regressor. These notes are especially important because even though the paper above represents the idea in depth, it doesn't actually contain how to implement Nu-SVR or how Mercer's Theorem works for regression [link](http://vision.psych.umn.edu/users/schrater/schrater_lab/courses/PattRecog09/RegressionII.pdf)\n",
    "4. MATLAB's documentation also has a good mathematical explanation for SVR [link](https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)\n",
    "\n",
    "\n",
    "We are done !! Let's use the algorithm..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0201e07",
   "metadata": {
    "id": "d0201e07"
   },
   "source": [
    "I will use the California housing dataset for the regression models that I will introduce. This is actually a very well-known dataset but it will probably be our first time using it. The aim is to predict median house prices and I will directly use sklearn to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d466b39",
   "metadata": {
    "id": "2d466b39"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4a223e",
   "metadata": {
    "id": "0f4a223e",
    "outputId": "dc6d2153-615b-470a-fc30-155dcc3e472f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = pd.DataFrame(data=california_housing.data, columns=california_housing.feature_names)\n",
    "predictors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897b4a4",
   "metadata": {
    "id": "6897b4a4"
   },
   "source": [
    "Let's scale the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c15bd83",
   "metadata": {
    "id": "4c15bd83"
   },
   "outputs": [],
   "source": [
    "predictors=StandardScaler().fit_transform(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e5299",
   "metadata": {
    "id": "114e5299"
   },
   "source": [
    "Let's also get the target feature and then we can use `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c8d8ae",
   "metadata": {
    "id": "42c8d8ae"
   },
   "outputs": [],
   "source": [
    "target=california_housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906d0976",
   "metadata": {
    "id": "906d0976"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9acb6a",
   "metadata": {
    "id": "8c9acb6a"
   },
   "source": [
    "## Linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea5987",
   "metadata": {
    "id": "16ea5987"
   },
   "source": [
    "I will first use `LinearSVR()` just like I used `SVC()` in the first notebook. We will tune almost the same parameters by using grid search but this time we will use insensitive epsilon loss instead of hinge loss. Moreover, for all the examples I will use `RandomizedSearchCV()` not to spend too much time on tuning. \n",
    "\n",
    "\n",
    "Documentation for LinearSVR: [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c78838cc",
   "metadata": {
    "id": "c78838cc"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f28a4b7",
   "metadata": {
    "id": "1f28a4b7"
   },
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    'C':[0.25,0.50,0.75,1,3,10,50,100],\n",
    "    'loss': [\"epsilon_insensitive\",\"squared_epsilon_insensitive\"],\n",
    "    'fit_intercept':[True,False],\n",
    "    'dual':[True,False],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca051e17",
   "metadata": {
    "id": "ca051e17"
   },
   "outputs": [],
   "source": [
    "rscv=RandomizedSearchCV(LinearSVR(), params1, cv=5,verbose=20,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f57139",
   "metadata": {
    "id": "97f57139",
    "outputId": "ba19d589-ad2f-47eb-e254-59128411f8e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde83449",
   "metadata": {
    "id": "fde83449",
    "outputId": "0bad8ca1-fa25-495e-e06c-f965f5f1252a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5551489167490848"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e550da1",
   "metadata": {
    "id": "8e550da1"
   },
   "source": [
    "The algorithm works well on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de9525",
   "metadata": {
    "id": "77de9525"
   },
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac7c1b",
   "metadata": {
    "id": "67ac7c1b"
   },
   "source": [
    "Now let's use `SVR()`. Hyperparameters are almost the same as `SVC()`, however, for SVR we also have the epsilon parameter that we talked about. Have a look at the documentation for learning more about the parameters [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76740296",
   "metadata": {
    "id": "76740296"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284cfdb9",
   "metadata": {
    "id": "284cfdb9"
   },
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    'C':[0.50,0.75,1,10,50],\n",
    "    'epsilon':[0.1,0.3,0.7],\n",
    "    'kernel': [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "    'gamma': [\"scale\",\"auto\"],\n",
    "    'degree':[2,3], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fca632c",
   "metadata": {
    "id": "8fca632c"
   },
   "outputs": [],
   "source": [
    "rscv=RandomizedSearchCV(SVR(), params2, cv=5,verbose=20,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cbc05",
   "metadata": {
    "id": "1f1cbc05",
    "outputId": "76da18f5-686c-4065-8c8f-a6e40cad2b87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa56532d",
   "metadata": {
    "id": "fa56532d",
    "outputId": "8569a599-6ff7-4d17-e1da-361c74b7fa39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35503059000030135"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30fa56",
   "metadata": {
    "id": "4b30fa56"
   },
   "source": [
    "SVR seems like performing better than LinearSVR which is actually something expected. Let's also use Nu-SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e1b15",
   "metadata": {
    "id": "e42e1b15"
   },
   "source": [
    "# Nu-SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916d8cf4",
   "metadata": {
    "id": "916d8cf4"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff29a9",
   "metadata": {
    "id": "01ff29a9"
   },
   "source": [
    "We have the same parameters as `NuSVC()`. \n",
    "\n",
    "Documentation [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f0c01c",
   "metadata": {
    "id": "c4f0c01c"
   },
   "outputs": [],
   "source": [
    "params3 = {\n",
    "    'nu':[0.25,0.50,0.75,0.95],\n",
    "    'kernel': [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "    'gamma': [\"scale\",\"auto\"],\n",
    "    'degree':[2,3], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc73097",
   "metadata": {
    "id": "ddc73097"
   },
   "outputs": [],
   "source": [
    "rscv=RandomizedSearchCV(NuSVR(), params3, cv=5,verbose=20,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d48f4",
   "metadata": {
    "id": "ee2d48f4",
    "outputId": "6e6aab51-3139-43f3-8b15-cf93175251d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375a3387",
   "metadata": {
    "id": "375a3387",
    "outputId": "91322c56-a82a-4762-e351-775c855ba911"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35577164624532537"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a29058",
   "metadata": {
    "id": "99a29058"
   },
   "source": [
    "NuSVR performs almost as well as SVR for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b77795",
   "metadata": {
    "id": "30b77795"
   },
   "source": [
    "# Other Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975fd9e",
   "metadata": {
    "id": "b975fd9e"
   },
   "source": [
    "In the [linear models documentation](https://scikit-learn.org/stable/modules/linear_model.html#) of sklearn, there are some regression algorithms that I didn't introduce. Instead of providing the mathematics and algorithm behind it, I will focus on briefly how to use these algorithms and share sources in which you can find necessary information about these algorithms. I was actually planning to make another notebook about these regression algorithms and include the math behind them but they are not often used, thus, I chose to briefly mention them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9df203",
   "metadata": {
    "id": "af9df203"
   },
   "source": [
    "## Least Angle Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62299dd",
   "metadata": {
    "id": "b62299dd"
   },
   "source": [
    "LARS is a stepwise regression algorithm that at each step tries to find the most correlated feature with the target and tunes the regression line towards the most correlated feature. You can find a good explanation of the algorithm on [GeeksforGeeks](https://www.geeksforgeeks.org/least-angle-regression-lars/). Moreover, there is a good explanation in one of Quora posts [link](https://www.quora.com/What-is-an-intuitive-explanation-for-least-angle-regression). However, If you'd like to read a more mathematical source I recommend this paper [Least Angle Regression](https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf). Let's use the algorithm.\n",
    "\n",
    "Sklearn provides us `Lars()` estimator for using LARS, however, I will use `LassoLars()` which combines lasso regularization and the LARS algorithm.\n",
    "\n",
    "You can find more about the algorithm in the linear model documentation that I shared above as well as in LassoLars documentation: [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13a91ee",
   "metadata": {
    "id": "e13a91ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLars # we will be using Lasso regularized version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0440d836",
   "metadata": {
    "id": "0440d836"
   },
   "outputs": [],
   "source": [
    "ll=LassoLars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e06f5f0",
   "metadata": {
    "id": "9e06f5f0"
   },
   "outputs": [],
   "source": [
    "params4 = {\n",
    "    'alpha':[0.25,0.50,0.75,1,3,5,10],\n",
    "    'positive': [True,False],\n",
    "    'normalize': [True,False],\n",
    "    'fit_intercept':[True,False], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c59e73f6",
   "metadata": {
    "id": "c59e73f6"
   },
   "outputs": [],
   "source": [
    "rscv=RandomizedSearchCV(ll, params4, cv=10,verbose=2,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dbbff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "3b2dbbff",
    "outputId": "cb80a0bd-cac8-4519-dfa8-ab647f659f00",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7401a22",
   "metadata": {
    "id": "c7401a22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389005423747067"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5c686",
   "metadata": {
    "id": "d6b5c686"
   },
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e29ef",
   "metadata": {
    "id": "8f0e29ef"
   },
   "source": [
    "Bayesian Regression is a regression algorithm that is based on Bayesian Inference. You can find a good explanation of it in the SVR paper that I shared above, there is a section about Bayesian Regression at the end of the paper. In addition to that, there are also lots of good sources for that algorithm.\n",
    "\n",
    "1. Ritvikmath has a video about this algorithm (In fact this video introduced me to this algorithm for the first time) [Bayesian Linear Regression : Data Science Concepts](https://www.youtube.com/watch?v=Z6HGJMUakmc).\n",
    "2. There are two well-written medium articles about that algorithm. [Bayesian Statistics Overview and your first Bayesian Linear Regression Model](https://towardsdatascience.com/bayesian-statistics-overview-and-your-first-bayesian-linear-regression-model-ba566676c5a7) and [Introduction to Bayesian Linear Regression](https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7). \n",
    "3. This is a very well-written introduction to Bayesian Regression in which you can find both practice and theory. [link](https://statswithr.github.io/book/introduction-to-bayesian-regression.html).\n",
    "4. GeeksforGeeks has a good article about this algorithm [link](https://www.geeksforgeeks.org/implementation-of-bayesian-regression/).\n",
    "5. Lastly, there are course notes [link](https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec19-slides.pdf).\n",
    "\n",
    "Sklearn documentation for Bayesian Regression [link](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html)\n",
    "\n",
    "Let's use the algorithm now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf690965",
   "metadata": {
    "id": "bf690965"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd75276d",
   "metadata": {
    "id": "dd75276d"
   },
   "outputs": [],
   "source": [
    "br=BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c07eaf",
   "metadata": {
    "id": "50c07eaf"
   },
   "outputs": [],
   "source": [
    "params5 = {\n",
    "    'alpha_1':np.geomspace(1e-9, 1e1, num=10),\n",
    "    'alpha_2':np.geomspace(1e-9, 1e1, num=10),\n",
    "    'lambda_1':np.geomspace(1e-9, 1e1, num=10),\n",
    "    'lambda_2':np.geomspace(1e-9, 1e1, num=10),\n",
    "    'normalize': [True,False],\n",
    "    'fit_intercept':[True,False], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db202335",
   "metadata": {
    "id": "db202335"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rscv=RandomizedSearchCV(br, params5, cv=10,verbose=2,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266e599",
   "metadata": {
    "id": "3266e599"
   },
   "outputs": [],
   "source": [
    "model=rscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efc49cf",
   "metadata": {
    "id": "4efc49cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5558505437321843"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(model.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538057d0",
   "metadata": {
    "id": "538057d0"
   },
   "source": [
    "## Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9f58e",
   "metadata": {
    "id": "66e9f58e"
   },
   "source": [
    "Lastly, we will use Quantile Regression. In the previous linear regression algorithms we used, there was a connection between the linear predictor and the mean or median. The quantile regression extends this idea and estimates any quantile that we are interested in. It generally estimates the 50th quantile which corresponds to the median, however, we can use it to estimate any quantile. \n",
    "\n",
    "For more about Quantile Regression:\n",
    "1. Firstly I recommend reading this Medium article [Quantile Regression](https://towardsdatascience.com/quantile-regression-ff2343c4a03)\n",
    "2. Secondly, This article introduces math in a more formal fashion [link](https://statisticaloddsandends.wordpress.com/2019/02/09/the-math-behind-quantile-regression/)\n",
    "3. Lastly you can look through this lecture notes [link](https://math.iupui.edu/~hanxpeng/Talks/Q_Reg.pdf)\n",
    "\n",
    "Sklearn Documentation for Quantile Regression: [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html#sklearn.linear_model.QuantileRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6078f2f0",
   "metadata": {
    "id": "6078f2f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e2bd9a",
   "metadata": {
    "id": "55e2bd9a"
   },
   "outputs": [],
   "source": [
    "qr=QuantileRegressor(alpha=0.6,quantile=0.75,fit_intercept=True,solver=\"highs-ipm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb48c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_reg=qr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "743fc1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.666814281709787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(quantile_reg.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731a162",
   "metadata": {},
   "source": [
    "You may wonder why I didn't use hyperparameter tuning. The drawback of Quantile Regression is that it allocates too much memory and using hyperparameter tuning makes it even worse. I actually tuned the model with `RandomizedSearchCV()` one time and got a 0.53 mean squared error but then I accidentally deleted the output. Since I think It's not a widely used algorithm I didn't spend another 6 hours tuning a quantile regression model. Moreover, I actually used this regression model on R a couple of times and even though there was a similar problem with R, I didn't have as much trouble as I did while using Quantile Regression on Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6252477",
   "metadata": {
    "id": "a6252477"
   },
   "source": [
    "# Comparison of Regression Models in This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e23e43",
   "metadata": {
    "id": "00e23e43"
   },
   "source": [
    "Let's also have a quick example and compare the regression algorithms in this notebook on the more real-world-like dataset. I will use the Facebook comment dataset and try to predict the number of comments. Let's get started\n",
    "\n",
    "I will first load the dataset. I downloaded the file from Kaggle [link](https://www.kaggle.com/kiranraje/prediction-facebook-comment). In fact, this dataset can be found on UCI Data Repository and I previously took it from there for another project but I found this version easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e30647",
   "metadata": {
    "id": "51e30647"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec089a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "6ec089a0",
    "outputId": "5705c3b0-da9b-48a4-8733-2d8a2fe8f5bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>Checkins</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Category</th>\n",
       "      <th>commBase</th>\n",
       "      <th>comm24</th>\n",
       "      <th>comm48</th>\n",
       "      <th>comm24_1</th>\n",
       "      <th>diff2448</th>\n",
       "      <th>baseTime</th>\n",
       "      <th>...</th>\n",
       "      <th>fri_pub</th>\n",
       "      <th>sat_pub</th>\n",
       "      <th>sun_base</th>\n",
       "      <th>mon_base</th>\n",
       "      <th>tue_base</th>\n",
       "      <th>wed_base</th>\n",
       "      <th>thu_base</th>\n",
       "      <th>fri_base</th>\n",
       "      <th>sat_base</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    likes  Checkins  Returns  Category  commBase  comm24  comm48  comm24_1  \\\n",
       "0  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "1  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "2  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "3  634995         0    463.0       1.0       7.0       0     3.0         7   \n",
       "4  634995         0    463.0       1.0       1.0       0     0.0         1   \n",
       "\n",
       "   diff2448  baseTime  ...  fri_pub  sat_pub  sun_base  mon_base  tue_base  \\\n",
       "0         0        65  ...        0        0         0       0.0         0   \n",
       "1         0        10  ...        0        0         0       0.0         0   \n",
       "2         0        14  ...        1        0         0       0.0         0   \n",
       "3        -3        62  ...        1        0         0       1.0         0   \n",
       "4         0        58  ...        0        0         0       0.0         0   \n",
       "\n",
       "   wed_base  thu_base  fri_base  sat_base  output  \n",
       "0         0         0         0         1       0  \n",
       "1         0         0         1         0       0  \n",
       "2         0         0         0         1       0  \n",
       "3         0         0         0         0       0  \n",
       "4         1         0         0         0       0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0817d7",
   "metadata": {
    "id": "1a0817d7"
   },
   "source": [
    "I will not use all the features while training the model. I chose the features that are used in this Kaggle [post](https://www.kaggle.com/code/vinayvk1808/facebook-comment-dataset-using-random-forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c3ae1f",
   "metadata": {
    "id": "d2c3ae1f"
   },
   "outputs": [],
   "source": [
    "predictors=data[['likes', 'Checkins', 'Returns', 'Category', 'commBase', 'comm24',\n",
    "       'comm48', 'comm24_1', 'diff2448', 'baseTime', 'length', 'hrs',\n",
    "       'sun_pub', 'tue_pub', 'wed_pub', 'fri_pub', 'sat_pub', 'sun_base',\n",
    "       'tue_base', 'wed_base', 'thu_base','fri_base']]\n",
    "target=data[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608798df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "608798df",
    "outputId": "e3669fbd-077a-43b8-8e36-55fdeb8bab44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>Checkins</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Category</th>\n",
       "      <th>commBase</th>\n",
       "      <th>comm24</th>\n",
       "      <th>comm48</th>\n",
       "      <th>comm24_1</th>\n",
       "      <th>diff2448</th>\n",
       "      <th>baseTime</th>\n",
       "      <th>...</th>\n",
       "      <th>sun_pub</th>\n",
       "      <th>tue_pub</th>\n",
       "      <th>wed_pub</th>\n",
       "      <th>fri_pub</th>\n",
       "      <th>sat_pub</th>\n",
       "      <th>sun_base</th>\n",
       "      <th>tue_base</th>\n",
       "      <th>wed_base</th>\n",
       "      <th>thu_base</th>\n",
       "      <th>fri_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>-3</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>634995</td>\n",
       "      <td>0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    likes  Checkins  Returns  Category  commBase  comm24  comm48  comm24_1  \\\n",
       "0  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "1  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "2  634995         0    463.0       1.0       0.0       0     0.0         0   \n",
       "3  634995         0    463.0       1.0       7.0       0     3.0         7   \n",
       "4  634995         0    463.0       1.0       1.0       0     0.0         1   \n",
       "\n",
       "   diff2448  baseTime  ...  sun_pub  tue_pub  wed_pub  fri_pub  sat_pub  \\\n",
       "0         0        65  ...        0        0        1        0        0   \n",
       "1         0        10  ...        0        0        0        0        0   \n",
       "2         0        14  ...        0        0        0        1        0   \n",
       "3        -3        62  ...        0        0        0        1        0   \n",
       "4         0        58  ...        0        0        0        0        0   \n",
       "\n",
       "   sun_base  tue_base  wed_base  thu_base  fri_base  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         1  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         1         0         0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b7ca3",
   "metadata": {
    "id": "7b5b7ca3"
   },
   "source": [
    "Now let's do some preprocessing. I will use `SimpleImputer()`, `StandardScaler()` and `PolynomialFeatures()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a0a6e5",
   "metadata": {
    "id": "58a0a6e5"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "predictors=SimpleImputer(strategy='median').fit_transform(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af11a5f3",
   "metadata": {
    "id": "af11a5f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "predictors_pf=PolynomialFeatures(degree=2).fit_transform(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7c2766",
   "metadata": {
    "id": "bb7c2766"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "predictors_scaled=StandardScaler().fit_transform(predictors_pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d0f590",
   "metadata": {
    "id": "e3d0f590"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors_scaled, target, test_size=0.2,random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292b80a",
   "metadata": {
    "id": "9292b80a"
   },
   "source": [
    "Since I will use all the regression algorithms in this notebook, I will define my own hyperparameter tuning function so as not to repeat similar code every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836e65e8",
   "metadata": {
    "id": "836e65e8"
   },
   "outputs": [],
   "source": [
    "def RegressorCV(estimator,parameters):\n",
    "    rscv=RandomizedSearchCV(estimator, parameters, cv=5,verbose=2,scoring=\"neg_mean_squared_error\");\n",
    "    model=rscv.fit(X_train,y_train)\n",
    "    mean_squared_error(model.predict(X_test),y_test)\n",
    "    results = {'MSE':mean_squared_error(y_test,rscv.predict(X_test)),\n",
    "           'Best_Model':rscv.best_params_,\n",
    "           'CV_results':pd.DataFrame(rscv.cv_results_),\n",
    "           'RSquared':r2_score(rscv.predict(X_test),y_test)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206e4a5",
   "metadata": {
    "id": "a206e4a5"
   },
   "source": [
    "Let's now use `RegressorCV()` with each regression algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497f983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c497f983",
    "outputId": "1449a2f3-3fd6-4436-c8ed-6ec60f9651e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "results_LinearSVR=RegressorCV(LinearSVR(),params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12061dc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12061dc3",
    "outputId": "939dcf25-2cb4-4a51-d234-6e1d1f9922a7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_SVR=RegressorCV(SVR(),params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db400b5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db400b5d",
    "outputId": "a5392ef4-89c0-4e53-d90a-f24f41afe041",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVR\n",
    "results_NuSVR=RegressorCV(NuSVR(),params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75faf1",
   "metadata": {
    "id": "2b75faf1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_LARS=RegressorCV(LassoLars(),params4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641baf13",
   "metadata": {
    "id": "641baf13",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_BR=RegressorCV(BayesianRidge(),params5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ac071",
   "metadata": {},
   "source": [
    "Let's show the results in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a03b5188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear SVR</th>\n",
       "      <th>SVR</th>\n",
       "      <th>Nu-SVR</th>\n",
       "      <th>LARS</th>\n",
       "      <th>Bayesian Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>574.724661</td>\n",
       "      <td>573.902483</td>\n",
       "      <td>566.157888</td>\n",
       "      <td>581.996072</td>\n",
       "      <td>514.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best Parameters</th>\n",
       "      <td>{'loss': 'epsilon_insensitive', 'fit_intercept': True, 'dual': True, 'C': 0.75}</td>\n",
       "      <td>{'kernel': 'linear', 'gamma': 'auto', 'epsilon': 0.3, 'degree': 3, 'C': 0.75}</td>\n",
       "      <td>{'nu': 0.25, 'kernel': 'linear', 'gamma': 'scale', 'degree': 2}</td>\n",
       "      <td>{'positive': False, 'normalize': False, 'fit_intercept': False, 'alpha': 0.5}</td>\n",
       "      <td>{'normalize': False, 'lambda_2': 10.0, 'lambda_1': 0.7742636826811278, 'fit_intercept': True, 'alpha_2': 2.1544346900318865e-06, 'alpha_1': 2.782559402207126e-05}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      Linear SVR                                                                            SVR                                                           Nu-SVR                                                                           LARS                                                                                                                                                 Bayesian Regression\n",
       "MSE                                                                                   574.724661                                                                     573.902483                                                       566.157888                                                                     581.996072                                                                                                                                                            514.9988\n",
       "Best Parameters  {'loss': 'epsilon_insensitive', 'fit_intercept': True, 'dual': True, 'C': 0.75}  {'kernel': 'linear', 'gamma': 'auto', 'epsilon': 0.3, 'degree': 3, 'C': 0.75}  {'nu': 0.25, 'kernel': 'linear', 'gamma': 'scale', 'degree': 2}  {'positive': False, 'normalize': False, 'fit_intercept': False, 'alpha': 0.5}  {'normalize': False, 'lambda_2': 10.0, 'lambda_1': 0.7742636826811278, 'fit_intercept': True, 'alpha_2': 2.1544346900318865e-06, 'alpha_1': 2.782559402207126e-05}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_vals = [results_LinearSVR[\"MSE\"],results_SVR[\"MSE\"],results_NuSVR[\"MSE\"],results_LARS[\"MSE\"],results_BR[\"MSE\"]]\n",
    "best_params=[results_LinearSVR[\"Best_Model\"],results_SVR[\"Best_Model\"],results_NuSVR[\"Best_Model\"],results_LARS[\"Best_Model\"],results_BR[\"Best_Model\"]]\n",
    "labels = {0:'Linear SVR',1:'SVR', 2:'Nu-SVR',3:'LARS',4:'Bayesian Regression'}\n",
    "columns={0:'MSE',1:'Best Parameters'}\n",
    "\n",
    "df =pd.DataFrame([mse_vals, best_params])\n",
    "df.rename(columns=labels, index=columns, inplace=True)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ad0f2",
   "metadata": {},
   "source": [
    "It looks like with the parameters we choose to optimize, Bayesian Regression performs better. However, All of these models actually don't perform well. We can tune the model further and engineer the data to get a better result. Nonetheless, I leave doing this to another notebook. Also, I didn't use quantile regression because of the memory problems I mentioned."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "name": "SVM4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
