
# Dimensionality Reduction Techniques on Python

## ðŸ‘‹ Introduction

Welcome to the sixth part of my repository, where we will explore the concept of dimensionality reduction and its application in machine learning. In this section, we will discuss various dimensionality reduction algorithms, including principal component analysis (PCA), linear discriminant analysis (LDA), and t-distributed stochastic neighbor embedding (t-SNE), among others.

## ðŸ—Š Contents:

**Dimensionality Reduction Techniques on Python - 1:** This notebook introduces the main concept behind dimensionality reduction and covers one of the most commonly used algorithms in this area, Principal Component Analysis (PCA). Additionally, we will explore variants of PCA, including Incremental PCA and Kernel PCA.

**Dimensionality Reduction Techniques on Python - 2:** In the second notebook, we extend our idea of dimensionality reduction with non-linear dimensionality reduction. In this notebook, I introduce Linear Discriminant Analysis (LDA), Multidimensional Scaling (MDS), ISOMAP, t-Distributed Stochastic Neighbor Embedding (t-SNE), and a variant of t-SNE called FFT-accelerated Interpolation-based t-SNE (Flt-SNE).  

**Dimensionality Reduction Techniques on Python - 3:** This notebook covers several dimensionality reduction algorithms, including Locally Linear Embedding (LLE), Laplacian Eigenmaps, and UMAP. Additionally, we will explore various utility functions of UMAP, such as UMAP plots.

**Dimensionality Reduction Techniques on Python - 4:** In the last notebook, we practice what we learnt in the previous three notebooks. In the first practice, we use t-SNE, Flt-SNE, and UMAP for genomic data visualization. In the second one, we use PCA and LLE to save some time for training Nu-SVM.


**Notes:** The `GridSearchCV()` and `RandomSearchCV()` results make the notebooks unnecessarily longer. Therefore, I deleted their outputs.
