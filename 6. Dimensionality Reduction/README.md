
# Dimensionality Reduction Techniques on Python
Welcome to the fifth part of my repository. In this part, we will talk about an algorithm called Naive Bayes Classifier which is an easy and straightforward application of the Bayes Theorem for classification.

## Contents:

**Dimensionality Reduction Techniques on Python - 1:** In this notebook, I introduce the main concept behind dimensionality reduction and most commonly used dimensionality reduction algorithm: Principal Component Analysis (PCA). I also introduce variants of PCA such as Incremental PCA and Kernel PCA.

**Dimensionality Reduction Techniques on Python - 2:** In the second notebook, we extend our idea of dimensionality reduction with non-linear dimensionality reduction. In this notebook, I introduce Linear Discriminant Analysis (LDA), Multidimensional Scaling (MDS), ISOMAP, t-Distributed Stochastic Neighbor Embedding (t-SNE), and a variant of t-SNE: FFT-accelerated Interpolation-based t-SNE (Flt-SNE).  

**Dimensionality Reduction Techniques on Python - 3:** In this notebook, I introduce Locally Linear Embedding (LLE), Laplacian Eigenmaps, and lastly UMAP. I also introduced utility functions of UMAP such as UMAP plots.

**Dimensionality Reduction Techniques on Python - 4:** In the last notebook, we practice what we learnt in the previous three notebooks. In the first practice, we use t-SNE, Flt-SNE and UMAP for genomic data visualization. In the second one, we use PCA and LLE to save some time for training Nu-SVM.


**Notes:** The `GridSearchCV()` and `RandomSearchCV()` results make the notebooks unnecessarily longer. Therefore, I deleted their outputs.
