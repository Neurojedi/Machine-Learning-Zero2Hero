{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afeddacf",
   "metadata": {},
   "source": [
    "# Linear Models in Machine Learning on Python - Logistic Regression 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773082c",
   "metadata": {},
   "source": [
    "Welcome to the last notebook about Logistic Regression. In the last notebook on Logistic Regression I will do two examples, one for binary classification and the other one is for multiclass classification. In the binary classification example, I will use the weather dataset that we used in the second notebook, but this time I will try to deal with class imbalance, by extension of that, I will try to improve the performance. In the second example, I will use a new dataset and introduce some new cross validation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b90872",
   "metadata": {},
   "source": [
    "## Binary Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28775e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b303d87",
   "metadata": {},
   "source": [
    "Let's load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"weatherAUS.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229ed2b",
   "metadata": {},
   "source": [
    "I will define the function that I utilized in the second notebook and then transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdate(data_s):\n",
    "    data_s[\"Date\"] = pd.to_datetime(data_s[\"Date\"]) #convert it to date\n",
    "    data_s[\"Year\"]=data_s[\"Date\"].dt.year # Parsing year \n",
    "    data_s[\"Month\"]=data_s[\"Date\"].dt.month # Parsing month\n",
    "    data_s[\"Day\"]=data_s[\"Date\"].dt.day # Parsing day\n",
    "    data_s =  data_s.drop([\"Date\"], axis=1) # Dropping original Date row\n",
    "    return data_s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Convert_Missing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,data): # no *args or **kargs\n",
    "        print(\"Transforming in progress...\")\n",
    "        self.data=data;\n",
    "        self.splitted_data=splitdate(data);\n",
    "        self.categorical = [features for features in self.splitted_data.columns if self.splitted_data[features].dtype==object]\n",
    "        self.numerical = [features for features in self.splitted_data.columns if self.splitted_data[features].dtype==float]\n",
    "        self.numerical_int64 = [features for features in self.splitted_data.columns if self.splitted_data[features].dtype=='int64']\n",
    "    def transform(self):\n",
    "        imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        features_categorical=imputer.fit_transform(self.splitted_data[self.categorical])\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        features_numeric=imputer.fit_transform(self.splitted_data[self.numerical])\n",
    "        concatenated_array=np.concatenate((features_categorical,features_numeric),axis=1)#concatane arrays obtained from imputer\n",
    "        dataframe=pd.DataFrame(concatenated_array, columns=self.categorical+self.numerical) # convert it to dataframe\n",
    "        frames = [dataframe, data[self.numerical_int64]] # combining the dataframe in the previous line and date dataframe \n",
    "        final_dataframe=pd.concat(frames,axis=1)  # concataneting the two dataframes in the previous line\n",
    "        final_dataframe=final_dataframe.loc[:,~final_dataframe.columns.duplicated()]\n",
    "        print(\"Transforming completed\")\n",
    "        return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61425d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "def One_Hot_Encoder(data):\n",
    "    numerical = [features for features in data.columns if data[features].dtype==float]  # getting names of numerical columns\n",
    "    categorical = [features for features in data.columns if data[features].dtype==object] # getting names of categorical columns\n",
    "    numerical_int64 = [features for features in data.columns if data[features].dtype=='int64']\n",
    "    data=pd.concat([data[numerical],data[categorical],data[numerical_int64],pd.get_dummies(data.Location), \n",
    "                     pd.get_dummies(data.WindGustDir),\n",
    "                     pd.get_dummies(data.WindDir9am),\n",
    "                     pd.get_dummies(data.WindDir3pm)],axis=1)\n",
    "    data[\"RainToday\"]=le.fit_transform(data[\"RainToday\"])\n",
    "    data.drop(['Location','WindGustDir','WindDir9am','WindDir3pm'],axis=1,inplace=True)\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleandmerge(data):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    numerical_data = scaler.fit_transform(data[numerical])\n",
    "    numerical_data = pd.DataFrame(numerical_data, columns=[numerical])\n",
    "    data.drop(numerical,axis=1,inplace=True)\n",
    "    data_processed=pd.concat([numerical_data, data],axis=1) \n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d87e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedprocessing(data):\n",
    "    missing_transformer=Convert_Missing(data)\n",
    "    data1=missing_transformer.transform()\n",
    "    data2=One_Hot_Encoder(data1)\n",
    "    data3=scaleandmerge(data2)\n",
    "    return data3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4417ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [features for features in data.columns if data[features].dtype==float]\n",
    "data_processed=combinedprocessing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a0faf",
   "metadata": {},
   "source": [
    "There is one important processing step that we need to do. When we are applying OneHotEncoding we also create some columns with the same name. This is not actually because of OneHotEncoding but because of the features that we encode have common values. For example let's look at the feature ******. If you look at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_column_uniquify(df):\n",
    "    df_columns = df.columns\n",
    "    new_columns = []\n",
    "    for item in df_columns:\n",
    "        counter = 0\n",
    "        newitem = item\n",
    "        while newitem in new_columns:\n",
    "            counter += 1\n",
    "            newitem = \"{}_{}\".format(item, counter)\n",
    "        new_columns.append(newitem)\n",
    "    df.columns = new_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed=df_column_uniquify(data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c1105",
   "metadata": {},
   "source": [
    "Now let's use train_test_split() function on processed data and encode our target feature with LabelEncoder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "le=LabelEncoder()\n",
    "target=le.fit_transform(data_processed[\"RainTomorrow\"])\n",
    "data_processed.drop([\"RainTomorrow\"],axis=1,inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed, target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d4a2e",
   "metadata": {},
   "source": [
    "## Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e982423",
   "metadata": {},
   "source": [
    "We are done with transforming the data, now let's again look at the class imbalance in our target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a652347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06620369",
   "metadata": {},
   "source": [
    "In this part I will talk about one of the ways to deal with class imbalance, resampling. There are other ways and we actually used some of them before(such as adding class weight to classifier). However, resampling is a more general step for dealing with class imbalance compared to other methods that we used previously. If you would like to learn more about other ways to deal with class imbalance you can use check the [link](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) \n",
    "\n",
    " \n",
    "In order to deal with class imbalance we can resample our dataset in two ways, we can either remove samples from the majority class (that is called undersampling) or we can add more examples from the minority class (that is called oversampling). There are different algorithms to oversample or undersample. In this part I will talk about some of them and train a model to select which of these techniques works best for our dataset. In the end I will share a depository that you can find more about these techniques and other techniques that you can use to resample. On the other hand, before starting, I would like to share an article about Statistical Sampling. It's an important concept to learn I think so that one can understand the importance of stratifiedsampling and sampling error, this article will be a good introduction for you [link](https://machinelearningmastery.com/statistical-sampling-and-resampling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc80f0",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49731f",
   "metadata": {},
   "source": [
    "The first approach that I will use for resampling is Random Undersampling. As name suggests, we will randomly select some instances from the dataset and then delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d047d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27318758",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy='majority',random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f280c",
   "metadata": {},
   "source": [
    "One important parameter that we will use for resampling functions is **sampling_strategy**. When we pass majority, it means that we want to reduce the majority class to the size of minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d14b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler_float = RandomUnderSampler(sampling_strategy=0.7,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014d904",
   "metadata": {},
   "source": [
    "We can also set the sampling_strategy to a floating point value. In this case we are actually setting a percantage. For example, setting the argument to 0.3 will cause our our majority class to have %30 more instances than the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rusmajority,yrusmajority = undersampler.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ce903",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(yrusmajority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c5212",
   "metadata": {},
   "source": [
    "Let's also use the second RandomUnderSampler() that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c01cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rusfloat, yrusfloat = undersampler_float.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(yrusfloat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825d2c6",
   "metadata": {},
   "source": [
    "You probably noticed that I only resample the training set. To understand why, read the discussion [link](https://datascience.stackexchange.com/questions/57882/resampling-for-imbalaced-datasets-should-testing-set-also-be-resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce1956",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a9c3f",
   "metadata": {},
   "source": [
    "I will do almost the exact same steps that I did above, with a little difference. This time I will set sampling_strategy to minority since we are randomly oversampling (copying random instances) the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy='minority',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ef524",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler_float = RandomOverSampler(sampling_strategy=0.4,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rosmajority,yrosmajority = oversampler.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(yrosmajority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rosfloat, yrosfloat = oversampler_float.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(yrosfloat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b9530",
   "metadata": {},
   "source": [
    "Documentation for `RandomOverSampler()` [link](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)\n",
    "\n",
    "\n",
    "Documentation for `RandomUnderSampler()` [link](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)\n",
    "\n",
    "Also I do recommend reading this article on machine learning mastery [link](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1142007",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241546d",
   "metadata": {},
   "source": [
    "One of the main problems of randomly sampling is it doesn't provide much additional information to the model. We are simply adding copies of the some random instances. Instead, we can use an algorithm called SMOTE that synthetically creates new instance in the minority class. The algorithm uses an approach that is very similar to a machine learning algorithm called K-Means, I will not go into details but I will share some sources where you can find the algorithm of the function (It's actually pretty easy).\n",
    "\n",
    "1. MachineLearningMastery article for SMOTE [link](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "\n",
    "2. This article has a good explanation for SMOTE and other resampling methods [link](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97898fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239182c2",
   "metadata": {},
   "source": [
    "The algorithm has almost the same parameters as `RandomOverSampler()` or `RandomUnderSampler()` except that the parameter **k_neighbors**. Briefly, while creating new samples the algorithm randomly picks an instance and then find closest instances to the selected one, then creates new instances between them. The parameter k_neighbors defines the distance that our algorithm uses for finding closest instances. For example, when it is set to 5 the algorithm find the closes five instances to create new ones between them. For learning more about this function check out the documentation [link](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy=0.7,k_neighbors=50)\n",
    "X_smote, y_smote = oversample.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f3cf4",
   "metadata": {},
   "source": [
    "### SMOTE with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "over = SMOTE(sampling_strategy=0.3)\n",
    "under = RandomUnderSampler(sampling_strategy=0.6)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ab837",
   "metadata": {},
   "source": [
    "Notice that we have a different pipeline in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote_rus, y_smote_rus = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_smote_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5dd4b",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32692671",
   "metadata": {},
   "source": [
    "When we look at the dataframe it's obvious that we have lots of features. There's an important processing step that we didn't talk about before, it's feature selection. I will talk about feature selection more when we start learning Support Vector Machines, however, let's now use a function that simply selects important features for us. Here I will use an algorithm called Recursive Feature Elimination, RFE in short. For using this algorithm we will use `RFE()` function in sklearn, and pass the machine learning algorithm that we will use in addition to number of features that we want. You can find a detailed explanation for the function on MachineLearningMastery [link](https://machinelearningmastery.com/rfe-feature-selection-in-python/) also don't forget to check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n",
    "Firstly, let's define RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "RFE_feaselector = RFE(estimator=LogisticRegression(C=1,fit_intercept=True, penalty=\"l2\", warm_start=True,class_weight=\"balanced\"),n_features_to_select= 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6a296",
   "metadata": {},
   "source": [
    "Now I will use `fit()` method on X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE_feaselector.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993af8e",
   "metadata": {},
   "source": [
    "After fitting, use `get_support()` method to get which columns to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = rfe.get_support(1)\n",
    "dataset_reduced = X_train[X_train.columns[selected_columns]] # reducing the size of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(x,y):\n",
    "    rfe = selector.fit(x,y)\n",
    "    selected_columns = rfe_model.get_support(1)\n",
    "    dataset_reduced = X_train[X_train.columns[selected_columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e0b21",
   "metadata": {},
   "source": [
    "We are ready to train our models on the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf22c4",
   "metadata": {},
   "source": [
    "## Training Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267428f9",
   "metadata": {},
   "source": [
    "I will use `GridSearchCV()` on every resampled X_train and y_train. I will do that using a for loop and for every model I will save performance metrics and best parameters.\n",
    "\n",
    "Let's firstly convert our training sets into a form that we can loop through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87856a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={1:[X_rusmajority,yrusmajority],\n",
    "         2:[X_rusfloat, yrusfloat],\n",
    "         3:[X_rosmajority,yrosmajority],\n",
    "         4:[X_rosfloat, yrosfloat],\n",
    "         5:[X_smote, y_smote],\n",
    "         6:[X_smote_rus, y_smote_rus]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafccfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C':[100, 10, 1.0, 0.1, 0.01],\n",
    "    'l1_ratio': [0.25,0.50, 0.75, 0.90],\n",
    "    'penalty': [\"l2\",\"l1\",\"elasticnet\"],\n",
    "    'fit_intercept':[True,False],\n",
    "    'warm_start':[True,False],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ddb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gscv=GridSearchCV(logreg, params, cv=10,verbose=2,scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7094c6",
   "metadata": {},
   "source": [
    "Now let's fit our model for every dataset variants we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores=[]\n",
    "accuracy_scores=[]\n",
    "best_parameters=[]\n",
    "for i in datasets:\n",
    "    \n",
    "    model=gscv.fit(datasets[i][0], datasets[i][1])\n",
    "    f1_scores.append(f1_score(y_test, model.predict(X_test)))\n",
    "    accuracy_scores.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "    best_parameters.append(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7138e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecc51d",
   "metadata": {},
   "source": [
    "Looks like we improved the metrics slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc031b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375aa9e3",
   "metadata": {},
   "source": [
    "We are done with Binary Classification but before getting on with Multiclass example I just want to share some important websites in which you can find more about resampling.\n",
    "\n",
    "1. Github page of imblearn liberary: You can find names of other algorithms that you can use for resampling [link](https://github.com/scikit-learn-contrib/imbalanced-learn)\n",
    "2. A very good article about how SMOTE works [link](https://towardsdatascience.com/introduction-to-synthetic-minority-over-sampling-technique-and-its-implementation-from-scratch-77593647c10d). Moreover, you can also directly read the paper on SMOTE. I don't generally prefer sharing research papers but this one is pretty easy I think, especially if you read the article on Medium you won't probably have much trouble reading this [paper](https://arxiv.org/pdf/1106.1813.pdf).\n",
    "3. Also don't forget to check this article from MachineLearningMastery [link](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53293693",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f5dc1",
   "metadata": {},
   "source": [
    "I was thinking of using a dataset in which we can practice preprocessing but I don't want to keep this notebook very long. Therefore, we will continue with a well-known toy dataset IRIS Flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f833d",
   "metadata": {},
   "source": [
    "Let's load the dataset and examine for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f183b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=pd.DataFrame(iris.data, columns=[iris.feature_names])\n",
    "target=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c25541",
   "metadata": {},
   "source": [
    "Looks like we have no problem with class imbalance. Before going on with constructing our LogisticRegression model, I will show two important cross validation functions: `RepeatedKFold()` and `LeaveOneOut()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2191ed",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaccc40",
   "metadata": {},
   "source": [
    "Let's get started with `RepeatedKFold()`. The function involves a basic procedure, instead of using KFold cross validation for one time here we repeat the general KFold() cross validation procedure several times. Afterwards, the function generate overall prediction error by taking mean of the prediction error for every repeat. \n",
    "\n",
    "\n",
    "Another cross validation function that we will use is `LeaveOneOut()`. This algorithm trains repeatedly the model on all the instances except one and then use the omitted instance for prediction in test dataset. In other words, For n number of instances we train n number of models and each time leave out one instance for test set to predict. You can directly guess that using LeaveOneOut() strategy takes time because we train as much number of models as there are in our dataset.\n",
    "\n",
    "I recommend reading MachineLearningMastery articles on both of these functions: [LeaveOneOut](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/) and [RepeatedKFold](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "repeated_kfold = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b277c",
   "metadata": {},
   "source": [
    "The function above means that we will have 5 folds in each run and 15 in total. For every repeat, we shuffle the dataset and create 5 non-overlapping folds to use KFold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loocv=LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb155960",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c53d6b",
   "metadata": {},
   "source": [
    "Let's now define our GridSearchCV parameters. I will use almost the same parameters above for tuning except that I will also use **multi_class** parameter since we can use both one-vs-all and multinomial versions of Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C':[100, 10, 1.0, 0.1, 0.01],\n",
    "    'l1_ratio': [0.25,0.50, 0.75, 0.90],\n",
    "    'penalty': [\"l2\",\"l1\",\"elasticnet\"],\n",
    "    'multi_class':[\"ovr\", \"multinomial\"],\n",
    "    'fit_intercept':[True,False],\n",
    "    'warm_start':[True,False],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe740c12",
   "metadata": {},
   "source": [
    "Now I will create three models and use three different cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ecf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv=GridSearchCV(logreg, params, cv=5,verbose=10,scoring=\"accuracy\")#StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9447f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=gscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(gscv.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea556ec4",
   "metadata": {},
   "source": [
    "Now I will use a function to estimate elapsed time in GridSearchCV. I took this function from a stackoverflow discussion [link](https://datascience.stackexchange.com/questions/29495/how-to-estimate-gridsearchcv-computing-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time(gscv):\n",
    "    mean_fit_time= gscv.cv_results_['mean_fit_time']\n",
    "    mean_score_time= gscv.cv_results_['mean_score_time']\n",
    "    n_splits  = gscv.n_splits_ #number of splits of training data\n",
    "    n_iter = pd.DataFrame(gscv.cv_results_).shape[0] #Iterations per split\n",
    "    return print(np.mean(mean_fit_time + mean_score_time) * n_splits * n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e90b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153c2dc",
   "metadata": {},
   "source": [
    "When we use StratifiedKFold with k=5, it more or less takes 18 seconds to complete the training. I tried to calculate the elapsed time by myself and it was around 20 seconds. I don't know actually why there is a difference but we can say that this calculate_time() function will show relatively the computing time difference between cross validation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e09013",
   "metadata": {},
   "source": [
    "Let's go on with `RepeatedKFold()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7937c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv=GridSearchCV(logreg, params, cv=repeated_kfold,verbose=10,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726538f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2=gscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756daa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75885cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(gscv.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd870159",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89545fe5",
   "metadata": {},
   "source": [
    "Let's use Leave-One-Out strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv=GridSearchCV(logreg, params, cv=loocv,verbose=10,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=gscv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c76329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(gscv.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b5ed4",
   "metadata": {},
   "source": [
    "Let's use `onefunctiontoplothemall()` that we defined in the previous notebook to plot the performance curves for the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ce9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onefunctiontoplothemall(y_predicted,y,plot_type):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    th  = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    y_score=label_binarize(y_predicted, classes=[0, 1, 2])\n",
    "    y_test_binarized=label_binarize(y, classes=[0, 1, 2])\n",
    "    n_classes = y_score.shape[1] \n",
    "    \n",
    "    if(plot_type==\"microavg_roc\"):\n",
    "        \n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        # Plot ROC curve\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]))\n",
    "        sns.lineplot(x=[0, 1], y=[0, 1],label=\"No skill classifier\",linestyle=\"dashed\")\n",
    "        plt.xlabel(\"Fall-out\",fontsize=20)\n",
    "        plt.ylabel(\"Recall\",fontsize=20)\n",
    "        plt.legend(loc=\"lower right\",prop={'size': 16})\n",
    "    \n",
    "    \n",
    "    if(plot_type==\"roc\"):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        # Plot ROC curve\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            \n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "            \n",
    "\n",
    "        sns.lineplot(x=[0, 1], y=[0, 1],label=\"No skill classifier\",linestyle=\"dashed\")    \n",
    "        plt.xlim([-0.05, 1.0])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel(\"Fall-out\",fontsize=20)\n",
    "        plt.ylabel(\"Recall\",fontsize=20)\n",
    "        plt.legend(loc=\"lower right\",prop={'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "onefunctiontoplothemall(gscv.predict(X_test),y_test,\"roc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c440fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onefunctiontoplothemall(gscv.predict(X_test),y_test,\"microavg_roc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8725200",
   "metadata": {},
   "source": [
    "We are almost end of the LogisticRegression notebooks. I would like to talk about some cross validation functions that are originated from the ones that we used in this notebook. I will not use them here but I recommend you to do that. I will be probably using them in the upcoming notebooks.\n",
    "\n",
    "1. RepeatedStratifiedKFold: This function is the combination of RepeatedKFold and StratifiedKFold. It performs RepeatedKFold algorithm and make sures that the folds are representative of the general population. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html)\n",
    "2. LeavePOut: We saw that the funtion `LeaveOneOut()` leave out one instance for test set in each iteration. On the other hand, LeavePOut leaves P number of instance for test dataset in each iteration. This function doesn't create non-overlapping folds unlike LeaveOneOut. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)\n",
    "\n",
    "Lastly, check the user guide of sklearn for Cross Validation functions. There are some functions that I didn't show such as `GroupKFold()` (It makes sures that instance in testing and training folds are different as well as all the test folds are different from each other). I do recommend checking the [documentation](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
